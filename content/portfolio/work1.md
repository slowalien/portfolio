---
date: "2022-10-05"
draft: false
image: img/portfolio/youtubelogo.jpg
showonlyimage: false
title: YouTubers' Interactions with Algorithmic Decisions
weight: 1
---

YouTubers felt confused and unfair about the opacity and inconsistency of algorithmic decisions they received, and these decisions impacted their performance metrics such as viewership, audience engagement, and ad income. This project provides **translatable design considerations** for algorithmic moderation systems creator/digital streaming platforms. 

<!--more-->

Videos:




#### Problem Statement

Content creators like YouTubers nowadays could be a profitable job for many people who aim to earn money (e.g., advertising income from YouTube or Creator Fund from TikTok) from platforms. For example, YouTube decides ad income from the viewing times and viewer engagement rates of videos, as the picture shown below.

[pic]

But not all video content is advertiser-friendly or acceptable to communities. When YouTube deems a YouTuber’s video as unacceptable, YouTube will demonetize (i.e., issuing “limited ads” or “no ads”) a YouTuber’s videos or channels, eventually denying them from earning more future ad revenue through placing limited or no ads on the videos.

As platform like YouTube uses various algorithms (e.g., machine learning) to implement content moderation, little attention has been paid to how users interact with algorithmic moderation and their post-hoc experience. Especially for the users perform profitable content creation, few studies have understand how they receive, perceive, and react to algorithmic content moderation. This study aims to approach this research gap in the context of algorithmic moderation on YouTube. We ask: **How do YouTubers interact with algorithmic moderation on YouTube?**

#### Methods
Interviewed **28 YouTubers who were in the YouTube Partner Program and had ever experienced moderation**.
We scraped online discussion data from the subreddit, r/youtube through a bag of keywords related to content moderation of YouTube. The final dataset contained 2,779 threads with 60,310 individual comments.
[pic]

#### Findings
[pic]

1. YouTubers experience **unequal moderation treatments** through cross-comparisons.
2. YouTubers observe that the moderation system makes **inconsistent decisions** or that it makes decisions **inconsistent with content policies**.
3. YouTubers **lack voice or control** in multiple algorithmic decision-making processes.


4. Algorithmic moderation engenders the **work uncertainty** of video content creation and how YouTubers manage their precarity.
5. YouTubers collectively shared and analyzed their punishment experiences to **speculate about moderation algorithms**, which in turn informed operations of repairing their past moderation and avoiding future moderation.

#### Design Considerations
> **Transparency** of algorithmic decision-making process such as disclosing whether YouTuber’s videos are invisible under ‘restricted mode,’ on Creator Studio dashboard.

>Informing creators of **how moderation decision affects performance metrics** such as income and visibility.

>**Creators’ voice** should be further valued by algorithmic assemblage of moderation decision-making (e.g., self-certification tool).
